steps to do project from Scratch
step 1: to generate datalake:
  for these we will be using AWS server, Glue for job and database connection and Athena for sql queries and s3 bucket for storage
  go to s3 bucket create 3 bucket(input, output and script)
    now go to glue:
      step1: create database
      step2: create crawler
      step3: create glue job (reference link: https://www.youtube.com/watch?v=8jlAoB1GmNs)
Step2: go to s3 bucket where transformed file is stored, download it and add csv extension and upload it s3 bucket
Step3: create emr cluster with m.5large core
once ready connect using ssh
Step4: in console download matplotlib, seaborn, pandas, boto3
      then create a new file using command::-   vi ur_name.py
      then add the script in the file and using command::-  :qw exit the file
      run the file with the command spark-submit ur_name.py
      check the output in the output_bucket(path specified in script)
Step5: terminate cluster
